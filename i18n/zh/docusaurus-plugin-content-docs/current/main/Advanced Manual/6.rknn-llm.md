---
sidebar_label: "6. RKLLM 简介"
sidebar_position: 7
slug: /advanced-manual/rknn-llm
---

# 6. RKLLM
## 6.1 RKLLM 简介
RKLLM 软件堆栈可以帮助用户快速将 AI 模型部署到 Rockchip 芯片上。 整体框架如下：

![rknn-llm](/img/general-tutorial/rknn-llm.png)

### 6.1.1 RKLLM 工具链介绍
#### 6.1.1.1 RKLLM-Toolkit 功能介绍

RKLLM-Toolkit 是为用户提供在计算机上进行大语言模型的量化、转换的开发套件。通过该
工具提供的 Python 接口可以便捷地完成以下功能：

1. 模型转换：支持将 Hugging Face 格式的大语言模型（Large Language Model, LLM）转换为
RKLLM 模型，目前支持的模型包括 LLaMA、Qwen/Qwen2、Phi2 等，转换后的 RKLLM 模型能
够在 Rockchip NPU 平台上加载使用。
2. 量化功能：支持将浮点模型量化为定点模型，目前支持的量化类型包括 w4a16 和 w8a8。

#### 6.1.1.2 RKLLM Runtime 功能介绍

RKLLM Runtime 主 要 负 责 加 载 RKLLM-Toolkit 转换得到的 RKLLM 模型，并在
RK3576/RK3588 板端通过调用 NPU 驱动在 Rockchip NPU 上实现 RKLLM 模型的推理。在推理
RKLLM 模型时，用户可以自行定义 RKLLM 模型的推理参数设置，定义不同的文本生成方式，
并通过预先定义的回调函数不断获得模型的推理结果。

### 6.1.2 RKLLM 开发流程介绍

RKLLM 的整体开发步骤主要分为 2 个部分：模型转换和板端部署运行。

1. 模型转换：
在这一阶段，用户提供的 Hugging Face 格式的大语言模型将会被转换为 RKLLM 格式，
以便在 Rockchip NPU 平台上进行高效的推理。这一步骤包括：
- a. 获取原始模型：获取 Hugging Face 格式的大语言模型；或是自行训练得到的大语言模
型，要求模型保存的结构与 Hugging Face 平台上的模型结构一致。
- b. 模型加载：通过 rkllm.load_huggingface()函数加载原始模型。
- c. 模型量化配置：通过 rkllm.build() 函数构建 RKLLM 模型，在构建过程中可选择是否
进行模型量化来提高模型部署在硬件上的性能，以及选择不同的优化等级和量化类型。
- d. 模型导出：通过 rkllm.export_rkllm() 函数将 RKLLM 模型导出为一个.rkllm 格式文件，
用于后续的部署。
2. 板端部署运行：
这个阶段涵盖了模型的实际部署和运行。它通常包括以下步骤：
- a. 模型初始化：加载 RKLLM 模型到 Rockchip NPU 平台，进行相应的模型参数设置来
定义所需的文本生成方式，并提前定义用于接受实时推理结果的回调函数，进行推理前准备。
- b. 模型推理：执行推理操作，将输入数据传递给模型并运行模型推理，用户可以通过预
先定义的回调函数不断获取推理结果。
- c. 模型释放：在完成推理流程后，释放模型资源，以便其他任务继续使用 NPU 的计算
资源。
这两个步骤构成了完整的 RKLLM 开发流程，确保大语言模型能够成功转换、调试，并最终
在 Rockchip NPU 上实现高效部署。

### 6.1.3 适用的硬件平台
本文档适用的硬件平台主要包括：**RK3576**、**RK3588**

## 6.2 开发环境准备

在发布的 RKLLM 工具链压缩文件中，包含了 RKLLM-Toolkit 的 whl 安装包、RKLLM 
Runtime 库的相关文件以及参考示例代码，具体的文件夹结构如下：
```
doc
└──Rockchip_RKLLM_SDK_CN.pdf # RKLLM SDK 说明文档
rkllm-runtime
├──example
│ └── src
│ └── main.cpp
│ └── build-android.sh
│ └── build-linux.sh
│ └── CMakeLists.txt
│ └── Readme.md
├──runtime
│ └── Android
│ └── librkllm_api
│ └──arm64-v8a
│ └── librkllmrt.so # RKLLM Runtime 库
│ └──include
│ └── rkllm.h # Runtime 头文件
│ └── Linux
│ └── librkllm_api
│ └──aarch64
│ └── librkllmrt.so
│ └──include
│ └── rkllm.h
rkllm-toolkit
├──examples
│ └── huggingface
│ └── test.py
├──packages
│ └── md5sum.txt 
│ └── rkllm_toolkit-1.0.0-cp38-cp38-linux_x86_64.whl
rknpu-driver
└──rknpu_driver_0.9.6_20240322.tar.bz2
```
在本章中将会对 RKLLM-Toolkit 工具及 RKLLM Runtime 的安装进行详细的介绍，具体的使
用方法请参考第 3 章中的使用说明。

### 6.2.1 RKLLM-Toolkit 安装

本节主要说明如何通过 pip 方式来安装 RKLLM-Toolkit，用户可以参考以下的具体流程说明
完成 RKLLM-Toolkit 工具链的安装。

#### 6.2.1.1 通过 pip 方式安装
##### 安装 miniforge3 工具

为防止系统对多个不同版本的 Python 环境的需求，建议使用 miniforge3 管理 Python 环境。
检查是否安装 miniforge3 和 conda 版本信息，若已安装则可省略此小节步骤。
```
conda -V
# 提示 conda: command not found 则表示未安装 conda
# 提示 例如版本 conda 23.9.0
```

下载 miniforge3 安装包

```
wget -c https://mirrors.bfsu.edu.cn/github-release/condaforge/miniforge/LatestRelease/Miniforge3-Linux-x86_64.sh
```

安装 miniforge3

```
chmod 777 Miniforge3-Linux-x86_64.sh
bash Miniforge3-Linux-x86_64.sh
```

##### 创建 RKLLM-Toolkit Conda 环境


进入 Conda base 环境
```
source ~/miniforge3/bin/activate # miniforge3 为安装目录
# (base) xxx@xxx-pc:~$
```
创建一个 Python3.8 版本（建议版本）名为 RKLLM-Toolkit 的 Conda 环境
```
conda create -n RKLLM-Toolkit python=3.8
```
进入 RKLLM-Toolkit Conda 环境
```
conda activate RKLLM-Toolkit
# (RKLLM-Toolkit) xxx@xxx-pc:~$
```

#### 6.2.1.2 安装 RKLLM-Toolkit

在 RKLLM-Toolkit Conda 环境下使用 pip 工具直接安装所提供的工具链 whl 包，在安装过程
中，安装工具会自动下载 RKLLM-Toolkit 工具所需要的相关依赖包。
```
pip3 install rkllm_toolkit-1.0.0-cp38-cp38-linux_x86_64.whl
```

若执行以下命令没有报错，则安装成功。
```
python
from rkllm.api import RKLLM
```

### 6.2.2 RKLLM Runtime 库的使用
在所公开的的 RKLLM 工具链文件中，包括包含 RKLLM Runtime 的全部文件：
- lib/librkllmrt.so: 适用于 RK3576/RK3588 板端调用进行 RKLLM 模型部署推理的
RKLLM Runtime 库；
- include/rkllm_api.h: 与 librkllmrt.so 函数库相对应的头文件，其中包含相关结构体及
函数定义的说明；
在通过 RKLLM 工具链构建 RK3576/RK3588 板端的部署推理代码时，需要注意对以上头文
件及函数库的链接，从而保证编译的正确性。当代码在 RK3576/RK3588 板端实际运行的过程中，
同样需要确保以上函数库文件成功推送至板端，并通过以下环境变量设置完成函数库的声明：
```
ulimit -Sn 50000
export LD_LIBRARY_PATH=./lib
./llm_demo qwen.rkllm
```

### 6.2.3 RKLLM Runtime 的编译要求

在使用 RKLLM Runtime 的过程中，需要注意 gcc 编译器的版本问题。推荐使用交叉编译工具
gcc-arm-10.2-2020.11-x86_64-aarch64-none-linux-gnu；具体的下载路径为：[GCC_10.2 交叉编译工
具下载地址](https://developer.arm.com/downloads/-/gnu-a/10-2-2020-11)。

请注意，交叉编译工具往往向下兼容而无法向上兼容，因此不要使用 10.2 以下的版本。

若是选择使用 Android 平台，需要进行 Android 可执行文件的编译，推荐使用 Android NDK
工具进行交叉编译，下载路径为：Android_NDK 交叉编译工具下载地址，推荐使用 r18b 版本。

具体的编译方式也可以参考 RKLLM-Toolkit 工具链文件中的 example/build_demo.sh。

### 6.2.4 芯片内核更新
由于当前公开的固件内核驱动版本不支持 RKLLM 工具，因此需要更新内核。rknpu 驱动包支持两
个主要内核版本：kernel-5.10 和 kernel-6.1。对于 kernel-5.10，建议使用具体版本号 5.10.198，repo：
GitHub - rockchip-linux/kernel at develop-5.10；对于 kernel-6.1，建议使用具体版本号 6.1.57。可在
内核根目录下的 Makefile 中确认具体版本号。
更新步骤如下：
```
a. 下载压缩包 rknpu_driver_0.9.6_20240322.tar.bz2。

b. 解压该压缩包，将其中的 rknpu 驱动代码覆盖到当前内核代码目录。

c. 重新编译内核。

d. 将新编译的内核烧录到设备中。
```

## 6.3 DeepSeek-R1 RK35XX部署

1. 下载DeepSeek-R1-1.5B HunggingFace 模型

* 新建一个目录，把这里所有文件下下来
[deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B at main](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/tree/main)

2. 编写转换脚本，放到deepseek模型目录里
```
from rkllm.api import RKLLM
from datasets import load_dataset
from transformers import AutoTokenizer
from tqdm import tqdm
import torch
from torch import nn
import os
# os.environ['CUDA_VISIBLE_DEVICES']='1'

modelpath = '.'
llm = RKLLM()

# Load model
# Use 'export CUDA_VISIBLE_DEVICES=2' to specify GPU device
# options ['cpu', 'cuda']
ret = llm.load_huggingface(model=modelpath, model_lora = None, device='cpu')
# ret = llm.load_gguf(model = modelpath)
if ret != 0:
    print('Load model failed!')
    exit(ret)

# Build model
dataset = "./data_quant.json"
# Json file format, please note to add prompt in the input，like this:
# [{"input":"Human: 你好！\nAssistant: ", "target": "你好！我是人工智能助手KK！"},...]

qparams = None
# qparams = 'gdq.qparams' # Use extra_qparams
#ret = llm.build(do_quantization=True, optimization_level=1, quantized_dtype='w8a8',
#                quantized_algorithm='normal', target_platform='rk3588', num_npu_core=3, extra_qparams=qparams, dataset=dataset)

ret = llm.build(do_quantization=True, optimization_level=1, quantized_dtype='w8a8',
                quantized_algorithm='normal', target_platform='rk3576', num_npu_core=2, extra_qparams=qparams, dataset=dataset)

if ret != 0:
    print('Build model failed!')
    exit(ret)

# Evaluate Accuracy
def eval_wikitext(llm):
    seqlen = 512
    tokenizer = AutoTokenizer.from_pretrained(
        modelpath, trust_remote_code=True)
    # Dataset download link:
    # https://huggingface.co/datasets/Salesforce/wikitext/tree/main/wikitext-2-raw-v1
    testenc = load_dataset(
        "parquet", data_files='./wikitext/wikitext-2-raw-1/test-00000-of-00001.parquet', split='train')
    testenc = tokenizer("\n\n".join(
        testenc['text']), return_tensors="pt").input_ids
    nsamples = testenc.numel() // seqlen
    nlls = []
    for i in tqdm(range(nsamples), desc="eval_wikitext: "):
        batch = testenc[:, (i * seqlen): ((i + 1) * seqlen)]
        inputs = {"input_ids": batch}
        lm_logits = llm.get_logits(inputs)
        if lm_logits is None:
            print("get logits failed!")
            return
        shift_logits = lm_logits[:, :-1, :]
        shift_labels = batch[:, 1:].to(lm_logits.device)
        loss_fct = nn.CrossEntropyLoss().to(lm_logits.device)
        loss = loss_fct(
            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        neg_log_likelihood = loss.float() * seqlen
        nlls.append(neg_log_likelihood)
    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * seqlen))
    print(f'wikitext-2-raw-1-test ppl: {round(ppl.item(), 2)}')

# eval_wikitext(llm)


# Chat with model
messages = "<|im_start|>system You are a helpful assistant.<|im_end|><|im_start|>user你好！\n<|im_end|><|im_start|>assistant"
kwargs = {"max_length": 128, "top_k": 1, "top_p": 0.8,
          "temperature": 0.8, "do_sample": True, "repetition_penalty": 1.1}
# print(llm.chat_model(messages, kwargs))


# Export rkllm model
ret = llm.export_rkllm("./deepseek-r1.rkllm")
if ret != 0:
    print('Export model failed!')
    exit(ret)
```
3. 如果需要做精度评估，就按代码里下载数据集做精度评估，如果需要调整量化策略，就在上头相应修改。执行该脚本，就能得到转换后的模型 deepseek-r1.rknn

4. 开发板部署运行（编写基于rkllm_api的开发板程序）

```
#include <cstdio>
#include <cstdint>
#include <cstdlib>
#include <cstring>
#include <string>
#include <iostream>
#include <fstream>
#include <vector>
#include <csignal>

#include "rkllm.h"

#define MODEL_PATH "/data/deepseek-r1_3588_w8a8.rkllm"

LLMHandle llmHandle = nullptr;

void exit_handler(int signal) {
    if (llmHandle != nullptr)
    {
        {
            std::cout << "程序即将退出" << std::endl;
            LLMHandle _tmp = llmHandle;
            llmHandle = nullptr;
            rkllm_destroy(_tmp);
        }
    }
    exit(signal);
}

void callback(RKLLMResult *result, void *userdata, LLMCallState state) {
    if (state == RKLLM_RUN_FINISH) {
        printf("\n");
    } else if (state == RKLLM_RUN_ERROR) {
        printf("\\run error\n");
    } else if (state == RKLLM_RUN_GET_LAST_HIDDEN_LAYER) {
        /* ================================================================================================================
        若使用GET_LAST_HIDDEN_LAYER功能,callback接口会回传内存指针:last_hidden_layer,token数量:num_tokens与隐藏层大小:embd_size
        通过这三个参数可以取得last_hidden_layer中的数据
        注:需要在当前callback中获取,若未及时获取,下一次callback会将该指针释放
        ===============================================================================================================*/
        if (result->last_hidden_layer.embd_size != 0 && result->last_hidden_layer.num_tokens != 0) {
            int data_size = result->last_hidden_layer.embd_size * result->last_hidden_layer.num_tokens * sizeof(float);
            printf("\ndata_size:%d",data_size);
            std::ofstream outFile("last_hidden_layer.bin", std::ios::binary);
            if (outFile.is_open()) {
                outFile.write(reinterpret_cast<const char*>(result->last_hidden_layer.hidden_states), data_size);
                outFile.close();
                std::cout << "Data saved to output.bin successfully!" << std::endl;
            } else {
                std::cerr << "Failed to open the file for writing!" << std::endl;
            }
        }
    } else if (state == RKLLM_RUN_NORMAL) {
        printf("%s", result->text);
    }
}

int main() {
    signal(SIGINT, exit_handler);
    printf("rkllm init start\n");

    //设置参数及初始化
    RKLLMParam param = rkllm_createDefaultParam();
    param.model_path = MODEL_PATH;

    //设置采样参数
    param.top_k = 1;
    param.top_p = 0.95;
    param.temperature = 0.8;
    param.repeat_penalty = 1.1;
    param.frequency_penalty = 0.0;
    param.presence_penalty = 0.0;

    param.max_new_tokens = 128000;
    param.max_context_len = 128000;
    param.skip_special_token = true;
    param.extend_param.base_domain_id = 0;

    int ret = rkllm_init(&llmHandle, ¶m, callback);
    if (ret == 0){
        printf("rkllm init success\n");
    } else {
        printf("rkllm init failed\n");
        exit_handler(-1);
    }

    std::string text;
    RKLLMInput rkllm_input;

    // 初始化 infer 参数结构体
    RKLLMInferParam rkllm_infer_params;
    memset(&rkllm_infer_params, 0, sizeof(RKLLMInferParam));  // 将所有内容初始化为 0

    rkllm_infer_params.mode = RKLLM_INFER_GENERATE;

    while (true)
    {
        std::string input_str;
        printf("\n");
        printf("user: ");
        std::getline(std::cin, input_str);
        if (input_str == "exit")
        {
            break;
        }

        text = input_str;
        rkllm_input.input_type = RKLLM_INPUT_PROMPT;
        rkllm_input.prompt_input = (char *)text.c_str();
        printf("robot: ");

        // 若要使用普通推理功能,则配置rkllm_infer_mode为RKLLM_INFER_GENERATE或不配置参数
        rkllm_run(llmHandle, &rkllm_input, &rkllm_infer_params, NULL);
    }
    rkllm_destroy(llmHandle);

    return 0;
}
```

:::tip
1. 如果你是copy自demo，请去掉场景提示词，deepseek不需要提示词。
2. 终端注意输入的汉字编码
3. 开发板内存需要8G或者8G以上，运行时候占用70%左右。
:::

quote:[部署 DeepSeek-R1 和 Janus-Pro](https://t.rock-chips.com/forum.php?mod=viewthread&tid=4836)

## 6.4 使用环境

开发板：[ArmSoM-Sige7](../armsom-sige7) / [ArmSoM-Sige5](../armsom-sige5)

代码仓库：[rknn-llm](https://github.com/ArmSoM/rknn-llm)

## 6.5 样品购买

- ArmSoM 独立站: [https://www.armsom.org/product-page/sige7](https://www.armsom.org/product-page/sige7)
 
- ArmSoM 速卖通官方店: [https://www.aliexpress.com/store/1102800175](https://www.aliexpress.com/store/1102800175) 

- ArmSoM 淘宝官方店: [https://item.taobao.com/item.htm?id=757023687970](https://item.taobao.com/item.htm?id=757023687970)

- OEM&ODM,  请联系: sales@armsom.org