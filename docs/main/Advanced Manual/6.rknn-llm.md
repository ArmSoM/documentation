---
sidebar_label: "6. RKLLM"
sidebar_position: 7
slug: /advanced-manual/rknn-llm
---

# 6. RKLLM
## 6.1 Introduction to RKLLM
The RKLLM software stack can help users quickly deploy AI models on Rockchip chips. The overall framework is as follows:

![rknn-llm](/img/general-tutorial/rknn-llm.png)

### 6.1.1 Introduction to RKLLM toolchain
#### 6.1.1.1 Introduction to RKLLM-Toolkit

RKLLM-Toolkit is a development kit that provides users with the ability to quantize and convert large language models on a computer. The Python interface provided by the tool can be used to easily complete the following functions:

1. Model conversion: Supports conversion of large language models (LLM) in Hugging Face format to RKLLM models. Currently supported models include LLaMA, Qwen/Qwen2, Phi2, etc. The converted RKLLM model can be loaded and used on the Rockchip NPU platform.
2. Quantization function: supports quantizing floating-point models to fixed-point models. Currently supported quantization types include w4a16 and w8a8.

#### 6.1.1.2 Introduction to RKLLM Runtime Function

RKLLM Runtime is mainly responsible for loading the RKLLM model converted by RKLLM-Toolkit, and implementing the inference of the RKLLM model on the Rockchip NPU by calling the NPU driver on the RK3576/RK3588 board. When inferring the RKLLM model, users can define the inference parameter settings of the RKLLM model, define different text generation methods, and continuously obtain the inference results of the model through pre-defined callback functions.

### 6.1.2 Introduction to RKLLM Development Process

The overall development steps of RKLLM are mainly divided into two parts: model conversion and board deployment and operation.

1. Model conversion:
In this stage, the large language model in Hugging Face format provided by the user will be converted to RKLLM format
for efficient inference on the Rockchip NPU platform. This step includes:
- a. Get the original model: Get the large language model in Hugging Face format; or the large language model trained by yourself, the structure of the model saved is required to be consistent with the model structure on the Hugging Face platform.
- b. Model loading: Load the original model through the rkllm.load_huggingface() function.
- c. Model quantization configuration: Build the RKLLM model through the rkllm.build() function. During the building process, you can choose whether to perform model quantization to improve the performance of the model deployed on the hardware, and choose different optimization levels and quantization types.
- d. Model export: Export the RKLLM model to a .rkllm format file through the rkllm.export_rkllm() function for subsequent deployment.
2. Board deployment and operation:
This stage covers the actual deployment and operation of the model. It usually includes the following steps:
- a. Model initialization: Load the RKLLM model to the Rockchip NPU platform, set the corresponding model parameters to define the required text generation method, and define the callback function for receiving real-time inference results in advance to prepare for inference.
- b. Model inference: Perform inference operations, pass input data to the model and run model inference. Users can continuously obtain inference results through pre-defined callback functions.
- c. Model release: After completing the inference process, release model resources so that other tasks can continue to use the computing resources of the NPU.
These two steps constitute the complete RKLLM development process, ensuring that large language models can be successfully converted, debugged, and ultimately deployed efficiently on the Rockchip NPU.

### 6.1.3 Applicable hardware platforms
The hardware platforms applicable to this document mainly include: **RK3576**, **RK3588**

## 6.2 Development environment preparation

The released RKLLM toolchain compressed file includes the whl installation package of RKLLM-Toolkit, the relevant files of RKLLM
Runtime library and reference sample code. The specific folder structure is as follows:
```
doc
└──Rockchip_RKLLM_SDK_CN.pdf # RKLLM SDK description document
rkllm-runtime
├──example
│ └── src
│ └── main.cpp
│ └── build-android.sh
│ └── build-linux.sh
│ └── CMakeLists.txt
│ └── Readme.md
├──runtime
│ └── Android
│ └── librkllm_api
│ └──arm64-v8a
│ └── librkllmrt.so # RKLLM Runtime library
│ └──include
│ └── rkllm.h # Runtime header file
│ └── Linux
│ └── librkllm_api
│ └──aarch64
│ └── librkllmrt.so
│ └──include
│ └── rkllm.h
rkllm-toolkit
├──examples
│ └── huggingface
│ └── test.py
├──packages
│ └── md5sum.txt
│ └── rkllm_toolkit-1.0.0-cp38-cp38-linux_x86_64.whl
rknpu-driver
└──rknpu_driver_0.9.6_20240322.tar.bz2
```
This chapter will introduce the installation of RKLLM-Toolkit and RKLLM Runtime in detail. For specific usage, please refer to the instructions in Chapter 3.

### 6.2.1 RKLLM-Toolkit Installation

This section mainly describes how to install RKLLM-Toolkit through pip. Users can refer to the following specific process instructions to complete the installation of RKLLM-Toolkit tool chain.

#### 6.2.1.1 Installation through pip
##### Install miniforge3 tool

To prevent the system from requiring multiple versions of Python environments, it is recommended to use miniforge3 to manage Python environments.
Check whether miniforge3 and conda version information are installed. If they are already installed, you can skip this step.
```
conda -V
# If the prompt conda: command not found indicates that conda is not installed
# Prompt, for example, version conda 23.9.0
```

Download miniforge3 installation package

```
wget -c https://mirrors.bfsu.edu.cn/github-release/conda forge/miniforge/LatestRelease/Miniforge3-Linux-x86_64.sh
```

Install miniforge3

```
chmod 777 Miniforge3-Linux-x86_64.sh
bash Miniforge3-Linux-x86_64.sh
```

##### Create RKLLM-Toolkit Conda environment

Enter Conda base environment
```
source ~/miniforge3/bin/activate # miniforge3 is the installation directory
# (base) xxx@xxx-pc:~$
```
Create a Python 3.8 version (recommended version) Conda environment named RKLLM-Toolkit
```
conda create -n RKLLM-Toolkit python=3.8
```
Enter the RKLLM-Toolkit Conda environment
```
conda activate RKLLM-Toolkit
# (RKLLM-Toolkit) xxx@xxx-pc:~$
```

#### 6.2.1.2 Install RKLLM-Toolkit

Use pip to directly install the provided toolchain whl package in the RKLLM-Toolkit Conda environment. During the installation process, the installation tool will automatically download the related dependency packages required by the RKLLM-Toolkit tool.
```
pip3 install rkllm_toolkit-1.0.0-cp38-cp38-linux_x86_64.whl
```

If there is no error when executing the following command, the installation is successful.
```
python
from rkllm.api import RKLLM
```

### 6.2.2 Use of RKLLM Runtime Library
The public RKLLM toolchain file includes all the files containing RKLLM Runtime:
- lib/librkllmrt.so: RKLLM Runtime library for RK3576/RK3588 board to call for RKLLM model deployment reasoning;
- include/rkllm_api.h: The header file corresponding to the librkllmrt.so function library, which contains the description of the relevant structure and function definition;
When building the deployment reasoning code for the RK3576/RK3588 board through the RKLLM toolchain, it is necessary to pay attention to the linking of the above header files and function libraries to ensure the correctness of the compilation. When the code is actually running on the RK3576/RK3588 board,
it is also necessary to ensure that the above function library files are successfully pushed to the board, and complete the declaration of the function library through the following environment variable settings:
```
ulimit -Sn 50000
export LD_LIBRARY_PATH=./lib
./llm_demo qwen.rkllm
```

### 6.2.3 Compilation requirements for RKLLM Runtime

When using RKLLM Runtime, you need to pay attention to the version of the gcc compiler. It is recommended to use the cross-compilation tool
gcc-arm-10.2-2020.11-x86_64-aarch64-none-linux-gnu; the specific download path is: [GCC_10.2 cross-compilation tool download address](https://developer.arm.com/downloads/-/gnu-a/10-2-2020-11).

Please note that cross-compilation tools are often backward compatible but not upward compatible, so do not use versions below 10.2.

If you choose to use the Android platform, you need to compile Android executable files. It is recommended to use the Android NDK tool for cross-compilation. The download path is: Android_NDK cross-compilation tool download address. It is recommended to use the r18b version.

For specific compilation methods, you can also refer to example/build_demo.sh in the RKLLM-Toolkit tool chain file.

### 6.2.4 Chip Kernel Update
Since the currently publicly available firmware kernel driver version does not support the RKLLM tool, the kernel needs to be updated. The rknpu driver package supports two main kernel versions: kernel - 5.10 and kernel - 6.1. For kernel - 5.10, the recommended specific version number is 5.10.198, and the repo is GitHub - rockchip - linux/kernel at develop - 5.10. For kernel - 6.1, the recommended specific version number is 6.1.57. You can confirm the specific version number in the Makefile at the root directory of the kernel.
The update steps are as follows:
```
a. Download the compressed package rknpu_driver_0.9.6_20240322.tar.bz2.

b. Unzip the compressed package and overwrite the rknpu driver code in it to the current kernel code directory.

c. Re - compile the kernel.

d. Burn the newly compiled kernel into the device.
```

## 6.3 DeepSeek - R1 RK35XX Deployment

1. Download the DeepSeek - R1 - 1.5B HunggingFace model
* Create a new directory and download all the files here.
[deepseek - ai/DeepSeek - R1 - Distill - Qwen - 1.5B at main](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/tree/main)

2. Write a conversion script and place it in the DeepSeek model directory.
```
from rkllm.api import RKLLM
from datasets import load_dataset
from transformers import AutoTokenizer
from tqdm import tqdm
import torch
from torch import nn
import os
# os.environ['CUDA_VISIBLE_DEVICES']='1'

modelpath = '.'
llm = RKLLM()

# Load model
# Use 'export CUDA_VISIBLE_DEVICES=2' to specify GPU device
# options ['cpu', 'cuda']
ret = llm.load_huggingface(model=modelpath, model_lora = None, device='cpu')
# ret = llm.load_gguf(model = modelpath)
if ret!= 0:
    print('Load model failed!')
    exit(ret)

# Build model
dataset = "./data_quant.json"
# Json file format, please note to add prompt in the input，like this:
# [{"input":"Human: 你好！\nAssistant: ", "target": "你好！我是人工智能助手KK！"},...]

qparams = None
# qparams = 'gdq.qparams' # Use extra_qparams
#ret = llm.build(do_quantization=True, optimization_level=1, quantized_dtype='w8a8',
#                quantized_algorithm='normal', target_platform='rk3588', num_npu_core=3, extra_qparams=qparams, dataset=dataset)

ret = llm.build(do_quantization=True, optimization_level=1, quantized_dtype='w8a8',
                quantized_algorithm='normal', target_platform='rk3576', num_npu_core=2, extra_qparams=qparams, dataset=dataset)

if ret!= 0:
    print('Build model failed!')
    exit(ret)

# Evaluate Accuracy
def eval_wikitext(llm):
    seqlen = 512
    tokenizer = AutoTokenizer.from_pretrained(
        modelpath, trust_remote_code=True)
    # Dataset download link:
    # https://huggingface.co/datasets/Salesforce/wikitext/tree/main/wikitext - 2 - raw - v1
    testenc = load_dataset(
        "parquet", data_files='./wikitext/wikitext - 2 - raw - 1/test - 00000 - of - 00001.parquet', split='train')
    testenc = tokenizer("\n\n".join(
        testenc['text']), return_tensors="pt").input_ids
    nsamples = testenc.numel() // seqlen
    nlls = []
    for i in tqdm(range(nsamples), desc="eval_wikitext: "):
        batch = testenc[:, (i * seqlen): ((i + 1) * seqlen)]
        inputs = {"input_ids": batch}
        lm_logits = llm.get_logits(inputs)
        if lm_logits is None:
            print("get logits failed!")
            return
        shift_logits = lm_logits[:, :-1, :]
        shift_labels = batch[:, 1:].to(lm_logits.device)
        loss_fct = nn.CrossEntropyLoss().to(lm_logits.device)
        loss = loss_fct(
            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        neg_log_likelihood = loss.float() * seqlen
        nlls.append(neg_log_likelihood)
    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * seqlen))
    print(f'wikitext - 2 - raw - 1 - test ppl: {round(ppl.item(), 2)}')

# eval_wikitext(llm)


# Chat with model
messages = "<|im_start|>system You are a helpful assistant.<|im_end|><|im_start|>user你好！\n<|im_end|><|im_start|>assistant"
kwargs = {"max_length": 128, "top_k": 1, "top_p": 0.8,
          "temperature": 0.8, "do_sample": True, "repetition_penalty": 1.1}
# print(llm.chat_model(messages, kwargs))


# Export rkllm model
ret = llm.export_rkllm("./deepseek - r1.rkllm")
if ret!= 0:
    print('Export model failed!')
    exit(ret)
```
3. If you need to perform accuracy evaluation, download the dataset as per the code and do the accuracy evaluation. If you need to adjust the quantization strategy, modify it accordingly above. Execute this script, and you can get the converted model deepseek - r1.rknn.

4. Deployment and running on the development board (write a development board program based on rkllm_api)
```
#include <cstdio>
#include <cstdint>
#include <cstdlib>
#include <cstring>
#include <string>
#include <iostream>
#include <fstream>
#include <vector>
#include <csignal>

#include "rkllm.h"

#define MODEL_PATH "/data/deepseek - r1_3588_w8a8.rkllm"

LLMHandle llmHandle = nullptr;

void exit_handler(int signal) {
    if (llmHandle!= nullptr)
    {
        {
            std::cout << "The program is about to exit" << std::endl;
            LLMHandle _tmp = llmHandle;
            llmHandle = nullptr;
            rkllm_destroy(_tmp);
        }
    }
    exit(signal);
}

void callback(RKLLMResult *result, void *userdata, LLMCallState state) {
    if (state == RKLLM_RUN_FINISH) {
        printf("\n");
    } else if (state == RKLLM_RUN_ERROR) {
        printf("\\run error\n");
    } else if (state == RKLLM_RUN_GET_LAST_HIDDEN_LAYER) {
        /* ================================================================================================================
        If the GET_LAST_HIDDEN_LAYER function is used, the callback interface will return the memory pointer: last_hidden_layer, the number of tokens: num_tokens, and the hidden layer size: embd_size.
        The data in last_hidden_layer can be obtained through these three parameters.
        Note: It needs to be obtained in the current callback. If not obtained in time, the next callback will release the pointer.
        ===============================================================================================================*/
        if (result->last_hidden_layer.embd_size!= 0 && result->last_hidden_layer.num_tokens!= 0) {
            int data_size = result->last_hidden_layer.embd_size * result->last_hidden_layer.num_tokens * sizeof(float);
            printf("\ndata_size:%d",data_size);
            std::ofstream outFile("last_hidden_layer.bin", std::ios::binary);
            if (outFile.is_open()) {
                outFile.write(reinterpret_cast<const char*>(result->last_hidden_layer.hidden_states), data_size);
                outFile.close();
                std::cout << "Data saved to output.bin successfully!" << std::endl;
            } else {
                std::cerr << "Failed to open the file for writing!" << std::endl;
            }
        }
    } else if (state == RKLLM_RUN_NORMAL) {
        printf("%s", result->text);
    }
}

int main() {
    signal(SIGINT, exit_handler);
    printf("rkllm init start\n");

    // Set parameters and initialize
    RKLLMParam param = rkllm_createDefaultParam();
    param.model_path = MODEL_PATH;

    // Set sampling parameters
    param.top_k = 1;
    param.top_p = 0.95;
    param.temperature = 0.8;
    param.repeat_penalty = 1.1;
    param.frequency_penalty = 0.0;
    param.presence_penalty = 0.0;

    param.max_new_tokens = 128000;
    param.max_context_len = 128000;
    param.skip_special_token = true;
    param.extend_param.base_domain_id = 0;

    int ret = rkllm_init(&llmHandle, ¶m, callback);
    if (ret == 0){
        printf("rkllm init success\n");
    } else {
        printf("rkllm init failed\n");
        exit_handler(-1);
    }

    std::string text;
    RKLLMInput rkllm_input;

    // Initialize the infer parameter structure
    RKLLMInferParam rkllm_infer_params;
    memset(&rkllm_infer_params, 0, sizeof(RKLLMInferParam));  // Initialize all contents to 0

    rkllm_infer_params.mode = RKLLM_INFER_GENERATE;

    while (true)
    {
        std::string input_str;
        printf("\n");
        printf("user: ");
        std::getline(std::cin, input_str);
        if (input_str == "exit")
        {
            break;
        }

        text = input_str;
        rkllm_input.input_type = RKLLM_INPUT_PROMPT;
        rkllm_input.prompt_input = (char *)text.c_str();
        printf("robot: ");

        // If you want to use the normal inference function, configure rkllm_infer_mode to RKLLM_INFER_GENERATE or do not configure parameters
        rkllm_run(llmHandle, &rkllm_input, &rkllm_infer_params, NULL);
    }
    rkllm_destroy(llmHandle);

    return 0;
}
```
:::tip
1. If you copy from the demo, remove the scene prompt words. DeepSeek does not need prompt words.
2. Pay attention to the Chinese character encoding of the input in the terminal.
3. The development board memory needs to be 8G or more, and it occupies about 70% during operation.
:::

quote: [Deploy DeepSeek - R1 and Janus - Pro](https://t.rock-chips.com/forum.php?mod=viewthread&tid=4836)

## 6.4 Usage Environment
Development board: [ArmSoM - Sige7](../armsom - sige7) / [ArmSoM - Sige5](../armsom - sige5)
Code repository: [rknn - llm](https://github.com/ArmSoM/rknn - llm)

## 6.5 Sample Purchase
ArmSoM independent website: [https://www.armsom.org/product - page/sige7](https://www.armsom.org/product - page/sige7)
ArmSoM AliExpress official store: [https://www.aliexpress.com/store/1102800175](https://www.aliexpress.com/store/1102800175) 
ArmSoM Taobao official store: [https://item.taobao.com/item.htm?id=757023687970](https://item.taobao.com/item.htm?id=757023687970)
For OEM & ODM, please contact: sales@armsom.org